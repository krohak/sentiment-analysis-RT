{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis using Neural Networks on the RT dataset\n",
    "created by krohak on 2018-03-10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first import necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/krohak/anaconda3/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import re\n",
    "\n",
    "from random import randint\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The explanation is divided into three parts. \n",
    "- In the first part, we outline how to create an IDs Matrix, which is a matrix containing the vector ID of each word in all files in the dataset. This code is provided in the `preprocess.py` file.\n",
    "- In the second part, we create our neural network and train it on the preprocessed data. This code is in the `train.py` file\n",
    "- In the third part, we test our neural network. `test.py` has the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For preprocessing the data, we need to create word vectors from each word, in each file of the positive and negative folders. We're going to be using [GloVe](http://nlp.stanford.edu/projects/glove/), a pretrained model which contains 400,000 word vectors. We will be using the [Wikipedia dataset with 50 dimensional embedding](https://www.damienpontifex.com/2017/10/27/using-pre-trained-glove-embeddings-in-tensorflow/).\n",
    "\n",
    "After performing `tar -xvzf data.tar.gz`, we get wordsList.npy, wordVectors.npy and idsMatrix2.py. wordsList is the list with the 400,000 words and wordVectors is a 400,000 x 50 dimensional embedding matrix that holds all of the word vector values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordsList = np.load('wordsList.npy')\n",
    "wordsList = wordsList.tolist() \n",
    "wordsList = [word.decode('UTF-8') for word in wordsList] \n",
    "wordVectors = np.load('wordVectors.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000\n",
      "(400000, 50)\n"
     ]
    }
   ],
   "source": [
    "print(len(wordsList))\n",
    "print(wordVectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before creating the ids matrix for the whole dataset, letâ€™s first take some time to visualize the type of data that we have. This will help us determine the best value for setting our maximum sequence length. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_sentoken/pos/cv091_7400.txt\n",
      "Positive files finished\n",
      "Negative files finished\n",
      "The total number of files is 2000\n",
      "The total number of words in the files is 1492681\n",
      "The average number of words in the files is 746.3405\n"
     ]
    }
   ],
   "source": [
    "positiveFiles = ['txt_sentoken/pos/' + f for f in listdir('txt_sentoken/pos/') if isfile(join('txt_sentoken/pos/', f))]\n",
    "negativeFiles = ['txt_sentoken/neg/' + f for f in listdir('txt_sentoken/neg/') if isfile(join('txt_sentoken/neg/', f))]\n",
    "print(positiveFiles[0])\n",
    "numWords = []\n",
    "for pf in positiveFiles:\n",
    "    with open(pf, \"r\", encoding='utf-8') as f:\n",
    "        lines=f.readlines()\n",
    "        counter = 0\n",
    "        for line in lines:\n",
    "            counter += len(line.split())\n",
    "        numWords.append(counter)\n",
    "print('Positive files finished')\n",
    "\n",
    "for nf in negativeFiles:\n",
    "    with open(nf, \"r\", encoding='utf-8') as f:\n",
    "        lines=f.readlines()\n",
    "        counter = 0\n",
    "        for line in lines:\n",
    "            counter += len(line.split())\n",
    "        numWords.append(counter)\n",
    "print('Negative files finished')\n",
    "\n",
    "numFiles = len(numWords)\n",
    "print('The total number of files is', numFiles)\n",
    "print('The total number of words in the files is', sum(numWords))\n",
    "print('The average number of words in the files is', sum(numWords)/len(numWords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2678"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(numWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the data in a histogram, so that we can compare the frequences and sequence length. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEKCAYAAAAIO8L1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/7G6JWCwoRMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7c05231c18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(numWords, 50)\n",
    "plt.xlabel('Sequence Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the histogram as well as the average number of words per file, we can safely say that most reviews will fall under 750 words, which is the max sequence length value we will set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxSeqLength = 750"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a single file and transform it into our ids matrix. This is what the first line of one of the reviews looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "txt_sentoken/pos/cv897_10837.txt\n",
      "i must admit that i was a tad skeptical of \" good will hunting \" , based both on the previews and the first fifteen minutes of the film , in which the main character will hunting ( matt damon ) , an mit janitor in his early twenties , is discovered to be an einstein-level closet genius when he solves two extraordinarily difficult math problems overnight . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "fname = positiveFiles[740] #Can use any valid index (not just 3)\n",
    "print(fname)\n",
    "with open(fname) as f:\n",
    "    for lines in f:\n",
    "        print(lines)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create a function for removing punctuation, parentheses, question marks, etc., and leave only alphanumeric characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "strip_special_chars = re.compile(\"[^A-Za-z0-9 ]+\")\n",
    "\n",
    "def cleanSentences(string):\n",
    "    string = string.lower().replace(\"<br />\", \" \")\n",
    "    return re.sub(strip_special_chars, \"\", string.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's convert to to an ids matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (750,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([     6,     42,     78,   2103,     88,      6,    853,   1650,\n",
       "            43,  11228,    101,    138,     17,      7,  57044,  19980,\n",
       "         11225,      3,    473,    417,   4311,      6,    637,  16574,\n",
       "          1483,     17,     26,   1095,     96,   3722,  49733,   2663,\n",
       "         25859,     14,   1140,    871,  18068,     83,   3779,   2241,\n",
       "        201534,    319,     31,    558,   2054,      3, 201534,   3880,\n",
       "          1388,     42,     15,     96,    647, 201534,   1247,     10,\n",
       "            47, 399999,      5,  20001,   1003,    143,      4, 201534,\n",
       "           380,      3,  16574,   1395,   1679,     64,     37,      4,\n",
       "        201534,    853,     12,    219,     43,   4429,    151,   6297,\n",
       "             4,   2933,    138,     29,  28687,   1751,   1945,    296,\n",
       "           881,  23808,  17614,      5,     81,    414,     30,   9215,\n",
       "           738,     41, 201398,    454,     37,    319,    133,   1569,\n",
       "           143,     10,     48,    873,     84,   1120,    197,    143,\n",
       "         10820,      5,  25859,     38,    836, 201534,   9045,      5,\n",
       "           369,  17788,   1461,  17354,   2933,     20,    138, 201534,\n",
       "           523,     14,    149,    871,  30761,      5,     36,   1113,\n",
       "           929,     63,     32,     52,      7,    306,   3468,     12,\n",
       "           119,    998,      7,   1594,  44004,    144], dtype=int32), None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "firstFile = np.zeros((maxSeqLength), dtype='int32')\n",
    "with open(fname) as f:\n",
    "    indexCounter = 0\n",
    "    lines=f.readlines()\n",
    "    for line in lines:\n",
    "        cleanedLine = cleanSentences(line)\n",
    "        split = cleanedLine.split()\n",
    "        #print(split)\n",
    "        for word in split:\n",
    "            if indexCounter < maxSeqLength:\n",
    "                try:\n",
    "                    firstFile[indexCounter] = wordsList.index(word)\n",
    "                except ValueError:\n",
    "                    firstFile[indexCounter] = 399999 #Vector for unknown words\n",
    "            indexCounter = indexCounter + 1\n",
    "firstFile[600:], print(\"shape:\", firstFile.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's do the same for each of our 2000 reviews in the Rotten Tomatoes dataset. \n",
    "\n",
    "Instead of computing the ids matrix, we can load in a pre-computed IDs matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ids = np.zeros((numFiles, maxSeqLength), dtype='int32')\n",
    "\n",
    "# fileCounter = 0\n",
    "\n",
    "# for pf in positiveFiles:\n",
    "#     with open(pf, \"r\") as f:\n",
    "#         indexCounter = 0\n",
    "#         lines=f.readlines()\n",
    "#         for line in lines:\n",
    "#             cleanedLine = cleanSentences(line)\n",
    "#             split = cleanedLine.split()\n",
    "#             for word in split:\n",
    "#                 try:\n",
    "#                     ids[fileCounter][indexCounter] = wordsList.index(word)\n",
    "#                 except ValueError:\n",
    "#                     ids[fileCounter][indexCounter] = 399999 #Vector for unkown words\n",
    "#                 print(fileCounter,indexCounter)\n",
    "#                 indexCounter = indexCounter + 1\n",
    "#                 if indexCounter >= maxSeqLength:\n",
    "#                     break\n",
    "#             if indexCounter >= maxSeqLength:\n",
    "#                     break\n",
    "#         fileCounter = fileCounter + 1\n",
    "\n",
    "# for nf in negativeFiles:\n",
    "#     with open(nf, \"r\") as f:\n",
    "#         indexCounter = 0\n",
    "#         lines=f.readlines()\n",
    "#         for line in lines:\n",
    "#             cleanedLine = cleanSentences(line)\n",
    "#             split = cleanedLine.split()\n",
    "#             for word in split:\n",
    "#                 try:\n",
    "#                     ids[fileCounter][indexCounter] = wordsList.index(word)\n",
    "#                 except ValueError:\n",
    "#                     ids[fileCounter][indexCounter] = 399999 #Vector for unkown words\n",
    "#                 indexCounter = indexCounter + 1\n",
    "#                 if indexCounter >= maxSeqLength:\n",
    "#                     break\n",
    "#             if indexCounter >= maxSeqLength:\n",
    "#                     break\n",
    "#         fileCounter = fileCounter + 1\n",
    "\n",
    "# np.save('idsMatrix2', ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load precomputed ids\n",
    "ids = np.load('idsMatrix2.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 750)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The RNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, weâ€™re ready to start creating our Tensorflow graph. Weâ€™ll first need to define some hyperparameters, such as batch size, number of LSTM units, number of output classes, and number of training iterations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batchSize = 24\n",
    "lstmUnits = 64\n",
    "numClasses = 2\n",
    "iterations = 100000\n",
    "numDimensions = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "labels = tf.placeholder(tf.float32, [batchSize, numClasses])\n",
    "input_data = tf.placeholder(tf.int32, [batchSize, maxSeqLength])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our input data placeholder, weâ€™re going to call the tf.nn.embedding_lookup() function in order to get our word vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tf.Variable(tf.zeros([batchSize, maxSeqLength, numDimensions]),dtype=tf.float32)\n",
    "data = tf.nn.embedding_lookup(wordVectors,input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weâ€™re then going to call the tf.nn.rnn_cell.BasicLSTMCell function.  Weâ€™ll then wrap that LSTM cell in a dropout layer to help prevent the network from overfitting. Finally, weâ€™ll feed both the LSTM cell and the 3-D tensor full of input data into a function called tf.nn.dynamic_rnn. This function is in charge of unrolling the whole network and creating a pathway for the data to flow through the RNN graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lstmCell = tf.contrib.rnn.BasicLSTMCell(lstmUnits)\n",
    "lstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, output_keep_prob=0.75)\n",
    "value, _ = tf.nn.dynamic_rnn(lstmCell, data, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first output of the dynamic RNN function can be thought of as the last hidden state vector. This vector will be reshaped and then multiplied by a final weight matrix and a bias term to obtain the final output values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "value = tf.transpose(value, [1, 0, 2])\n",
    "last = tf.gather(value, int(value.get_shape()[0]) - 1)\n",
    "\n",
    "weight = tf.Variable(tf.truncated_normal([lstmUnits, numClasses]))\n",
    "bias = tf.Variable(tf.constant(0.1, shape=[numClasses]))\n",
    "\n",
    "prediction = (tf.matmul(last, weight) + bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, weâ€™ll define correct prediction and accuracy metrics to track how the network is doing. The correct prediction formulation works by looking at the index of the maximum value of the 2 output values, and then seeing whether it matches with the training labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correctPred = tf.equal(tf.argmax(prediction,1), tf.argmax(labels,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correctPred, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weâ€™ll define a standard cross entropy loss with a softmax layer put on top of the final prediction values. For the optimizer, weâ€™ll use Adam and the default learning rate of .001. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=labels))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorboard to visualize the loss and accuracy values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.summary.scalar('Loss', loss)\n",
    "tf.summary.scalar('Accuracy', accuracy)\n",
    "merged = tf.summary.merge_all()\n",
    "logdir = \"tensorboard/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "with tf.Session() as sess:\n",
    "    writer = tf.summary.FileWriter(logdir, sess.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions to get the training and testing batches. Training is done on 4/5 of the dataset and testing on 1/5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getTrainBatch():\n",
    "    labels = []\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    for i in range(batchSize):\n",
    "        if (i % 2 == 0):\n",
    "            num = randint(1,979)\n",
    "            labels.append([1,0])\n",
    "        else:\n",
    "            num = randint(1019,1999)\n",
    "            labels.append([0,1])\n",
    "        arr[i] = ids[num-1:num]\n",
    "    return arr, labels\n",
    "\n",
    "def getTestBatch():\n",
    "    labels = []\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    for i in range(batchSize):\n",
    "        num = randint(979,1019)\n",
    "        if (num <= 999):\n",
    "            labels.append([1,0])\n",
    "        else:\n",
    "            labels.append([0,1])\n",
    "        arr[i] = ids[num-1:num]\n",
    "    return arr, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the model by loading in a batch of reviews and their associated labels. We compute the optimizer since that is the component that minimizes our loss function.\n",
    "\n",
    "Instead of training the network we can also load in a pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sess = tf.InteractiveSession()\n",
    "# saver = tf.train.Saver()\n",
    "# sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# for i in range(iterations):\n",
    "#    #Next Batch of reviews\n",
    "#    nextBatch, nextBatchLabels = getTrainBatch();\n",
    "#    sess.run(optimizer, {input_data: nextBatch, labels: nextBatchLabels})\n",
    "   \n",
    "#    #Write summary to Tensorboard\n",
    "#    if (i % 50 == 0):\n",
    "#        summary = sess.run(merged, {input_data: nextBatch, labels: nextBatchLabels})\n",
    "#        writer.add_summary(summary, i)\n",
    "\n",
    "#    #Save the network every 10,000 training iterations\n",
    "#    if (i % 10000 == 0 and i != 0):\n",
    "#        save_path = saver.save(sess, \"models/pretrained_lstm.ckpt\", global_step=i)\n",
    "#        print(\"saved to %s\" % save_path)\n",
    "# writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the Pretrained Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/pretrained_lstm.ckpt-60000\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "saver = tf.train.import_meta_graph('models/pretrained_lstm.ckpt-60000.meta')\n",
    "saver.restore(sess,tf.train.latest_checkpoint('models'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the graph definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gradients/rnn/while/basic_lstm_cell/BiasAdd_grad/tuple/control_dependency', 'gradients/rnn/while/basic_lstm_cell/BiasAdd_grad/tuple/control_dependency_1', 'gradients/rnn/while/basic_lstm_cell/MatMul_grad/MatMul', 'gradients/rnn/while/basic_lstm_cell/MatMul_grad/MatMul/Enter', 'gradients/rnn/while/basic_lstm_cell/MatMul_grad/MatMul_1', 'gradients/rnn/while/basic_lstm_cell/MatMul_grad/MatMul_1/f_acc', 'gradients/rnn/while/basic_lstm_cell/MatMul_grad/MatMul_1/Enter', 'gradients/rnn/while/basic_lstm_cell/MatMul_grad/MatMul_1/StackPushV2', 'gradients/rnn/while/basic_lstm_cell/MatMul_grad/MatMul_1/StackPopV2', 'gradients/rnn/while/basic_lstm_cell/MatMul_grad/MatMul_1/StackPopV2/Enter', 'gradients/rnn/while/basic_lstm_cell/MatMul_grad/tuple/group_deps', 'gradients/rnn/while/basic_lstm_cell/MatMul_grad/tuple/control_dependency', 'gradients/rnn/while/basic_lstm_cell/MatMul_grad/tuple/control_dependency_1', 'gradients/rnn/while/basic_lstm_cell/BiasAdd/Enter_grad/b_acc', 'gradients/rnn/while/basic_lstm_cell/BiasAdd/Enter_grad/b_acc_1', 'gradients/rnn/while/basic_lstm_cell/BiasAdd/Enter_grad/b_acc_2', 'gradients/rnn/while/basic_lstm_cell/BiasAdd/Enter_grad/Switch', 'gradients/rnn/while/basic_lstm_cell/BiasAdd/Enter_grad/Add', 'gradients/rnn/while/basic_lstm_cell/BiasAdd/Enter_grad/NextIteration', 'gradients/rnn/while/basic_lstm_cell/BiasAdd/Enter_grad/b_acc_3', 'gradients/rnn/while/basic_lstm_cell/concat_grad/Rank', 'gradients/rnn/while/basic_lstm_cell/concat_grad/mod', 'gradients/rnn/while/basic_lstm_cell/concat_grad/mod/Const', 'gradients/rnn/while/basic_lstm_cell/concat_grad/Shape', 'gradients/rnn/while/basic_lstm_cell/concat_grad/Shape_1', 'gradients/rnn/while/basic_lstm_cell/concat_grad/ConcatOffset', 'gradients/rnn/while/basic_lstm_cell/concat_grad/Slice', 'gradients/rnn/while/basic_lstm_cell/concat_grad/Slice_1', 'gradients/rnn/while/basic_lstm_cell/concat_grad/tuple/group_deps', 'gradients/rnn/while/basic_lstm_cell/concat_grad/tuple/control_dependency', 'gradients/rnn/while/basic_lstm_cell/concat_grad/tuple/control_dependency_1', 'gradients/rnn/while/basic_lstm_cell/MatMul/Enter_grad/b_acc', 'gradients/rnn/while/basic_lstm_cell/MatMul/Enter_grad/b_acc_1', 'gradients/rnn/while/basic_lstm_cell/MatMul/Enter_grad/b_acc_2', 'gradients/rnn/while/basic_lstm_cell/MatMul/Enter_grad/Switch', 'gradients/rnn/while/basic_lstm_cell/MatMul/Enter_grad/Add', 'gradients/rnn/while/basic_lstm_cell/MatMul/Enter_grad/NextIteration', 'gradients/rnn/while/basic_lstm_cell/MatMul/Enter_grad/b_acc_3', 'gradients/rnn/while/Switch_4_grad_1/NextIteration', 'beta1_power/initial_value_1', 'beta1_power_1', 'beta1_power/Assign_1', 'beta1_power/read_1', 'beta2_power/initial_value_1', 'beta2_power_1', 'beta2_power/Assign_1', 'beta2_power/read_1', 'rnn/basic_lstm_cell/kernel/Adam/Initializer/zeros_1', 'rnn/basic_lstm_cell/kernel/Adam_2', 'rnn/basic_lstm_cell/kernel/Adam/Assign_1']\n"
     ]
    }
   ],
   "source": [
    "graph = tf.get_default_graph()\n",
    "print([op.name for op in graph.get_operations()][1100:1150])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3 - Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the getTestBatch() function to generate test batches to test our model like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for this batch: 50.0\n",
      "Accuracy for this batch: 58.3333313465\n",
      "Accuracy for this batch: 54.1666686535\n",
      "Accuracy for this batch: 54.1666686535\n",
      "Accuracy for this batch: 58.3333313465\n",
      "Accuracy for this batch: 66.6666686535\n",
      "Accuracy for this batch: 54.1666686535\n",
      "Accuracy for this batch: 41.6666656733\n",
      "Accuracy for this batch: 50.0\n",
      "Accuracy for this batch: 54.1666686535\n"
     ]
    }
   ],
   "source": [
    "iterations = 10\n",
    "for i in range(iterations):\n",
    "    nextBatch, nextBatchLabels = getTestBatch();\n",
    "    print(\"Accuracy for this batch:\", (sess.run(accuracy, {input_data: nextBatch, labels: nextBatchLabels})) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also provide our own reviews / sentences and see how our model classifies its sentiment! A few helper functions to clean up the provided text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleanSentences(string):\n",
    "    string = string.lower().replace(\"<br />\", \" \")\n",
    "    return re.sub(strip_special_chars, \"\", string.lower())\n",
    "\n",
    "def getSentenceMatrix(sentence):\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    sentenceMatrix = np.zeros([batchSize,maxSeqLength], dtype='int32')\n",
    "    cleanedSentence = cleanSentences(sentence)\n",
    "    split = cleanedSentence.split()\n",
    "    for indexCounter,word in enumerate(split):\n",
    "        try:\n",
    "            sentenceMatrix[0,indexCounter] = wordsList.index(word)\n",
    "        except ValueError:\n",
    "            sentenceMatrix[0,indexCounter] = 399999 #Vector for unkown words\n",
    "    return sentenceMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can provide our own input text like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputText = \"Worst movie of all times. Bad screenplay, pathetic acting\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can provide an input text from another dataset. This one is a negative review from the [IMDB dataset](http://ai.stanford.edu/~amaas/data/sentiment/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# inputText = '''Nine minutes of psychedelic, pulsating, often symmetric abstract images, \n",
    "# are enough to drive anyone crazy. I did spot a full-frame eye at the start, \n",
    "# and later some birds silhouetted against other colors. It was just not my cup of tea. \n",
    "# It's about 8 minutes too long.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputMatrix = getSentenceMatrix(inputText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative Sentiment\n"
     ]
    }
   ],
   "source": [
    "predictedSentiment = sess.run(prediction, {input_data: inputMatrix})[0]\n",
    "\n",
    "if (predictedSentiment[0] > predictedSentiment[1]):\n",
    "    print(\"Positive Sentiment\")\n",
    "else:\n",
    "    print(\"Negative Sentiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try a positive review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "secondInputText = \"Interstellar is the best movie I have ever seen. The sotryline is gripping and intense\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can input one from the IMDB dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# secondInputText = '''When I was a kid I watched this many times over, \n",
    "# and I remember whistling the \"Happy Cat\" song quite often. \n",
    "# All the songs are great, and actually meorable, unlike many children's musicals, \n",
    "# where the songs are just stuck in for no real reason. The scenes and costumes are lavish, \n",
    "# and the acting is very well-done, which isn't surprising, considering the cast. \n",
    "# I'd recommend this film to children and parents alike, who love magic and fairytales. \n",
    "# And it actually IS a movie you can watch together, as it won't drive adults up the wall.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "secondInputMatrix = getSentenceMatrix(secondInputText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Sentiment\n"
     ]
    }
   ],
   "source": [
    "predictedSentiment = sess.run(prediction, {input_data: secondInputMatrix})[0]\n",
    "if (predictedSentiment[0] > predictedSentiment[1]):\n",
    "    print(\"Positive Sentiment\")\n",
    "else:\n",
    "    print(\"Negative Sentiment\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
